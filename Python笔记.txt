1.os.listdir(root)的作用：root是一个代表文件目录地址的字符串，该函数将会返回一个迭代器，可以获取root指向的
目录下所有文件的名称。常于for in 结构混搭

2.os.path.join(a,b,c,...)该函数将会返回一个文件地址，由a，b，c。。。拼接而成；

3.sorted(iterable, cmp=None, key=None, reverse=False)
sorted不会修改原来的可迭代对象
此函数将会返回一个新的可迭代对象，key 和 reverse 比一个等价的 cmp 函数处理速度要快。
这是因为对于每个列表元素，cmp 都会被调用多次，而 key 和 reverse 只被调用一次

4.Python中is关键字将会比较两个元素的内存地址是否相等，而==则是比较二者的内容是否一致

5.在同一个py文件中，除非明确操作分离，两个变量如果内容一样，内存就一样；

6.Dataset的魔法方法__getitem__的意义：
在用 for..in.. 迭代对象时，如果对象没有实现 __iter__ __next__ 迭代器协议，Python的解释器就会去
寻找__getitem__ 来迭代对象，如果连__getitem__ 都没有定义，这解释器就会报对象不是迭代器的错误，
所以__getitem__方法是用来实现迭代的，并且这个东西可以利用并行加速，性质比较优秀

7.一般来说可迭代对象优先使用__iter__和__next__这两个魔法方法来实现（这两个称之为迭代器协议）
如果定义了iter，则说明对象可迭代，如果有next，说明对象可以通过下标来索引

8.Python的enumerate()函数用于一个可迭代对象，同时返回该对象的元素和下标

9.Python中的type()函数用于返回指定对象的类名，注意不是返回DataType！！！

10.Pytorch中，如果只用load来加载模型：
句法应为：net = torch.load(path)
若用load_state_dict则是执行net.load_state_dict(torch.load(path))
注意，最好是保存module或者optimizer的.state_dict（）,而不是直接保存整个模型

11.python的time库：
time.time():返回当前时间戳
time.ctime():返回一个人类可以读懂的表示时间的字符串
time.strftime(tpl,ts)：按照tpl中的字符串模板自动填写当前的时间
time.sleep(s):睡眠s秒
time.perf_counter():可以连续调用两次求差值，查询过了多久时间，单位为秒

12.Python的__init__文件作用：
一：指示该文件夹是一个包，使得其他文件可以import该包
二：可以在__init__文件中提前import好需要的库，这样调用这个包的时候会自动import

13.从接口的角度来讲，对tensor的大部分操作同时支持torch.func和tensor.func两种表达方式

14.Python的函数三种参数：arg，*arg，**kwarg，后两个不是常规的参数。
*arg：将会把用户输入的参数打包成一个元组，供程序读取
**kwarg：将会把用户的输入打包成一个字典
注意，在函数定义的时候，这三者的定义顺序是确定的(arg,*arg,**kwarg),如果打乱将会报错

15.nn.ModuleList()没有实现forward方法，必须手动定义forward方法
nn.Sequential()实现了forward方法，并且输入可以是OrderedDict

16.注意，Python中初始化集合需要用set()，参数可以是列表，元组，字典等等，集合中的元素无序且不重，
集合不能用x = {}来初始化！

17.Python字典的操作函数.get（key,default）,根据键key返回对应的值，若找不到则返回default

18.Python在字符串前加u，是为了让这个字符串以unicode编码，防止因为字符串中有中文而乱码

19.Python字符串格式化中，一个方法是“{1}，{2}”.format(,),format中按123顺序给出{}中的内容
这个也可以用另一种方式，可在format中以赋值的方式指定标记位的内容，比如： （{1}，{2}.format（1 = "真滴流批" ,2 =  "没毛病" ））

20.Python的getattr(obj,name)为在指定实例对象obj中查找name属性

21.将tensor转化为ndarray，是用x.array()，而不是np.array(x)之类的其他命令

22.在linux环境下没有IDLE，就可以用ipdb库来进行debug，如果windows下就用PyCharm就好了

23.Python的字典类型有一个方法叫dict.item()，将会返回一个可遍历的元组数组

24.用函数hasattr(obj,name)返回实例对象obj中是否有name属性

25.字符串小技巧：str.startwith(str2)检查字符串str是否以str2开头

26.以双下划线开头的类属性是私有属性，不能被直接访问，也不能被子类对象访问，只能通过类中
定义的接口访问

27.以单下划线开头的类属性是保护对象，只有类对象和子类对象能访问到这些数据

28.在定义是用self.前缀来定义的属性是实例属性，每一个实例不同，而不带self.前缀的属性是类属性，是
类属性

29.Google的fire 命令行：python file <function> [args,] {--kwargs,}可以很方便的执行指定Python
文件的函数，输出return的结果，不需要设计输入输出接口，很舒服

30.Python的warning模块有一个函数warn函数，手动设置警报内容

31.Python的@修饰符：https://www.runoob.com/w3cnote/python-func-decorators.html(有关如果被修饰函数有参数该如何处理见此处)
例程：
def funA(fn):
   fn()
   print("It's funA")
 
@funA
def funC():
   print("It's funC")

那么执行funC()的结果就是：It's funCIt's funA，因为程序等价于funC = funA(funC)，当然修饰符可以嵌套，修饰符只是一个简化表达
那么此时funC就不再是一个函数，由于funA没有返回值，所以funC什么都不是，funC这个变量名在被修饰后变为什么取决于funA的返回值
那么那两个字符串会在程序运行到@funA语句时打印出来(也就是funA（funC）是在运行@funA时运行)

32.DataLoader的函数定义如下： DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, 
collate_fn=default_collate, pin_memory=False, drop_last=False)

dataset：加载的数据集(Dataset对象)
batch_size：batch size
shuffle:：是否将数据打乱
sampler： 样本抽样，后续会详细介绍
num_workers：使用多进程加载的进程数，0代表不使用多进程
collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可
pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些
drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃

33.Python的del关键字可用于删除变量或者函数或者数组元素，如果用赋值语句将函数赋给一个新的变量，然后删除旧函数，新变量所指代的
函数仍然能正常运行，相当于新变量申请了新内存

34.Python中函数内部可以定义函数(内部的函数外界无法访问)；函数可以作为输出返回出来(很jb有用！可以根据实际情况挑选函数)
函数还可以作为参数传进函数内部

35.函数名后加(),则是执行这个函数，若不加则是将函数视为变量，可以传来传去

36.Python定义类时，如果要在实例方法中修改类属性，需要用classname.classattr来引用，否则报错
事实上还不知道自由方法和静态方法存在的意义在哪里。。。
静态方法的修饰符是@staticmethod，不加self作为参数，可以理解为在类内部定义的一个函数，随便拿来用就行，
类方法的修饰符是@classmethod，加self作为参数
这两个方法都可以使用类属性，不可以使用实例属性和实例方法。
这样的规定和面向对象编程的内存管理规则有关。

37.sys.getrefcount()可以返回指向该目标的指针个数+1(因为函数本身也创立了一个指针指向目标)  

38.与zip相反，zip(*)可理解为解压，返回几个列表             

39.Python的csv操作： 
首先需要打开一个文件，与C语言不同，但与c++类似，
用with open（"filename",option）as f：的方式打开，f指代已被打开的文件，
然后创建csv.writer(fileobj)对象，也就是指定在f上写数据的对象，然后用这个写数据的对象的
方法比如writerow(),writerows()
而csv.reader则是用来读取的函数，不是一个对象，是一个元素为每一行的内容的列表
还支持将字典自动填写成二维表的操作
csv的特点：1、以行为单位读取；2，数据之间无空行，否则无法读取；3，读取出的数据一般为字符类型

40.为什么PyTorch中的forward方法是通过<实例对象>(input)来调用的？因为该实例对象实现了__call__(input)方法，__call__方法
可以接收上述函数的输入并调用自己，pytorch中正是通过__call__方法调用的forward方法

41.model.named_parameters()返回一个由（名字，参数）元组构成的列表

42.model.parameters()以列表形式返回所有的参数

43.Pytorch的squeeze()函数将会去掉数据中所有维数为1的维度，对维数不为一的维度不影响
而unsqueeze()则是在指定位置上加上一个维数为1的维度

44.Python的torchnet库中的ConfusionMeter对象存储的混淆矩阵形如：
 \ |  数据被程序分到哪一类
—|—————————
数|
据|
事|
实|
上|
应|
该|
被|
分|
到|
哪|
一|
类|
如果调用ConfusionMeter的value()方法，则会返回一个“正方形”的ndarray，这样就可以

45.optimizer.params_group:返回一个列表，每一个列表元素是一个字典，代表一份参数配置，由于我们可以在定义优化器
时定义不同的网络层拥有不同的训练参数，所以列表的长度可以大于1

50.Python的try—except语句：如果你觉得一部分代码可能出错，可以用这个句式在出错时跳过它

51.Python断言：assert 后跟一个布尔表达式
assert <boolean>:
	func....
将会在布尔表达式为false时直接返回错误，而不是等程序崩溃后再返回错误，方便定位bug

52.np.ravel()将一个多维的ndarray展成一维

53.np.linang.norm求范数

54.python字符串前加b意思是bytes,那么想要正确访问和操作这个字符串就必须一直用b'str'来书写，b不能丢

55.Python将数字转换为字符串的方法：str = “%d”%num'

56.Python面向对象，加__前缀的是private类型的，加_前缀的是protected类型的。

57.如果在Python类中调用此类中的任何方法或者实例属性（类属性不算），都必须要加self，解释器才能找得到。

58.tqdm库：必须还得from tqdm import tqdm才能调用tqdm函数

59.torchnet的meter库，加入值是add()，显示结果是value(),返回一个元组

60.Pytorch中对tensor重设尺寸是用.view，而不是reshape，这是numpy的函数

61.写BatchNorm层的时候要注意带上eps参数

62.Python面向对象中也讲究一个命名空间----只有类属性是对所有方法都可见的，带self.修饰的是对所有实例方法都可见的，但是对于类方法和
静态方法不可见。所有的规则都来自JAVA，只是表达上有区别。

63.Python报错"shadows built in"指命名和内置函数重名

64.如果想引用一个模块里的所有函数，就使用from module import *
不推荐大量使用
Python的第三方库的搜索顺序：
1.本目录
2.搜索系统变量PYTHONPATH下的目录
3.默认目录

65.Python设置函数内的变量为全局变量：global关键字

66.字符串前加r，字符串中的转义字符将全部失效变为纯字符串

67.Python的eval函数的输入必须是字符串

68.Pytorch的模型都有内部比较合理的初始化策略，一般不用我们考虑，写出来就好了

69.如果所有的训练图片都按类分文件夹放置好了，那么就可以考虑使用ImageFolder方法来处理dataset

70.numpy中转换数据类型的函数是.astype("<datatype>")，而Pytorch中是tensor.type("<datatype>")
而tensor.type_as(another_tensor)的作用是生成一个新的尺寸相同当时数据类型和another_tensor一样的tensor

pytorch中t.new_tensor()是一个有用的函数，创建新的内存分区，生成全新tensor。

Pytorch中调用.shape()查看tensor的尺寸时，生成的是一个独特的数据torch.Size([......])，所以想要判断tensor尺寸是否合要求
不能将shape和一个list比较。可以用torch.Size()来生成一个对应的数据。Tensor.Size()方法返回的值和Tensor.shape一样

71.Python中的filter(func,list)不是一个函数哦，是一个filter对象，作用是根据func中的返回值(返回为True的留下)筛选list中的数据，
需要用list()来将其变为一个list对象，不能直接使用

72.BatchNorm2d层不能在线性层使用哦，因为BatchNorm2d只接受由三维tensor组成的4维输入，可以用BatchNorm1d。

73.tensor.to()不能直接写tensor.to("cuda"),而应该tensor.to(torch.device("cuda"))，且需要使用赋值语句，否则原tensor
仍然在cpu上

一般训练过程中只需要将model和data和label放到gpu上即可

74.小技巧：如果我们需要调用另一个module中定义的类（假如是config.py中的config类），我们完全可以在config.py中
直接写opt = config（），然后在别的文件中使用from config.py import opt，这样就可以完美的回避可能遭遇的命名空间问题。

75.接上条：Python的奇妙技巧：
在Python中定义一个类时，可以在先类中引用一个类指针，在类定义完毕后再初始化这个指针。
举例如下：
class config:
    def parseConfig(self, **kwargs):
        if t.cuda.is_available():
            opt.device = t.device('cuda') ------？

opt = config()------！
这样不会编译错误！！
非常的神奇！暂时不知道啥原理，据检测java不支持这样的操作。c++懒得试了

76.Python一个文件中的任何函数，变量（即使x = 1）,类都可以被import到别的文件里去，和java的引用机制稍有区别。

77.记得把use_incoming_socket给关了。。。开着的话曲线会抽搐

78.fire用法：直接写python main.py func [**kwargs]即可，与linux命令行格式一样

79.加号+只能用于两个字符串间的连接，不能用于字符串和其他变量的连接，虽然一般也不是啥严重的错误就是了

80.Python的warnings库可以在指定条件下跳出自定义警告信息，使用也非常简单，只需要在指定的时候执行
warnings.warn(<str>)即可。

81.GOOGLE colab上切换工作路径的指令是%cd，而不是！cd，这是这个命令的特殊之处

82.Python fire 库的使用中，如果要传参数，建议以下格式：
python  main.py  <func name>  -<argu1 name>=argu1,<argu2 name>=argu2(参数个数可变)
要注意的一点是，根本不需要担心传入的参数还需要从字符串转换为实际可用的值。fire库会自动帮我们完成。注意输入参数的时候需要加
“。。。。。。”来告诉fire这个参数规定为字符串。

83.Python用open来打开文件的时候，要注意newline参数，如果不设置为“”直接用writerow会导致行与行之间有空行

84.在list之前加*，用于将list拆开为一个一个分开的变量，可以用于传给**kwargs的参数

85.orderdict的定义格式为OrderedDict([(a,1),(b,2),(c,3)])

86.调用model_state_dict()函数返回的只是内部参数的一个副本，直接修改是无效的。

87.update建议不要用于字典的单个成员的赋值，效率比较低

88.Pytorch接口小总结：
设model为一个继承自nn.Module的网络。
model.modules()：返回一个生成器，生成器生成model中的所有的子module，会将子module中的所有layer都给出来，
是一个迭代地拆分的过程
model.children(): 返回一个生成器，生成器生成model中的所有的子module，但不会迭代拆开封装了许多layer的子module
model.parameter():返回一个生成器，生成器的每一个元素是torch.nn.parameter.Parameter类的实例，需要用
.data才能以tensor形式访问参数，这时也就可以用等号赋值的方式修改参数的值了。
model.state_dict():返回一个OrderedDict，返回model中所有参数的一个副本
nn.Module.weight:返回一个装有参数的容器，必须用nn.Module.weight.data才能将其转换为tensor。注意对封装好的
网络不能调用weight。

如果要freeze网络中的参数，需要对parameter操作来freeze，而不是freeze layers，只有freeze每一层的参数才能奏效

89.z = nn.Sequential()中的layer可以通过加索引号z[i]来单独获取其中的层，从而达到精确修改网络的目的

90.tensor.copy(src):将src指向的tensor复制一份过来覆盖原tensor，要求两者的shape一致

91.python的set()函数将一个对象转换为集合对象，具有去重的功能，比如说set(String)就可以提取字符串中所有字符并去重。

92.用[for  in]结构可以构建列表，类似用{for in}可以创建字典。

93.numpy的随机矩阵：np.random.randn(),torch则不需要random。

95.ndarray是不能用作字典的键的：not hashable

96.python使用全局变量，需要在函数外部初始化，在函数内部再用global关键字声明一下即可

97.ndarray.ravel()将多维的ndarray转换为一维张量

98.字典的内部的架构是hashmap，在增删改这方面的性能远远优于列表，所以如果只需要存储索引关系，建议
用字典，将字典的键设置为下标，达到与列表同等的效果

99.用join连接字符串的时候，join(不是列表，而是**kwargs)